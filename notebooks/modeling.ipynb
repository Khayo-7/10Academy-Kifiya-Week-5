{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importib import reload\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\amseg-2.3-py3.12.egg\\amseg\\amharicSegmenter.py:55: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\amseg-2.3-py3.12.egg\\amseg\\amharicSegmenter.py:79: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\amseg-2.3-py3.12.egg\\amseg\\amharicSegmenter.py:55: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\amseg-2.3-py3.12.egg\\amseg\\amharicSegmenter.py:79: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\amseg-2.3-py3.12.egg\\amseg\\amharicNormalizer.py:87: SyntaxWarning: invalid escape sequence '\\s'\n",
      "c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\amseg-2.3-py3.12.egg\\amseg\\amharicNormalizer.py:87: SyntaxWarning: invalid escape sequence '\\s'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:10:17 - WARNING - From c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2025-01-23 14:10:17 - WARNING - From c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2025-01-23 14:10:17 - WARNING - From c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from scripts.modeling.model_utils import split_and_save_datasets\n",
    "from scripts.modeling.interpretability import explain_predictions\n",
    "from scripts.modeling.trainer import train_ner_model, fine_tune_multiple_models\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('..', 'resources', 'data')\n",
    "MODEL_PATH = os.path.join('..', 'resources', 'models')\n",
    "labeled_dir = os.path.join(DATA_PATH, 'labeled')\n",
    "splitted_dir = os.path.join(DATA_PATH, 'splitted')\n",
    "model_output_dir = os.path.join(MODEL_PATH, \"checkpoints\", \"ner_model\")\n",
    "os.makedirs(splitted_dir, exist_ok=True)\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "dataset_path = \"\"\n",
    "filepath = os.path.join(labeled_dir, 'labeled_messages.conll')\n",
    "\n",
    "\n",
    "# Model and Data Constants\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "MODEL_CHECKPOINT = \"xlm-roberta-base\"\n",
    "LABEL_LIST = [\"O\", \"B-Product\", \"I-Product\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_save_datasets(filepath, splitted_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:11:53 - INFO - [INFO] Loading tokenized dataset from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading tokenized dataset from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading tokenized dataset from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading tokenized dataset from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading tokenized dataset from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading tokenized dataset from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading tokenized dataset from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading metadata from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading metadata from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading metadata from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading metadata from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading metadata from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading metadata from disk.\n",
      "2025-01-23 14:11:53 - INFO - [INFO] Loading metadata from disk.\n",
      "2025-01-23 14:11:53 - INFO - Loading data from JSON file: ..\\resources\\data\\tokenized_dataset\\metadata.json\n",
      "2025-01-23 14:11:53 - INFO - Loading data from JSON file: ..\\resources\\data\\tokenized_dataset\\metadata.json\n",
      "2025-01-23 14:11:53 - INFO - Loading data from JSON file: ..\\resources\\data\\tokenized_dataset\\metadata.json\n",
      "2025-01-23 14:11:53 - INFO - Loading data from JSON file: ..\\resources\\data\\tokenized_dataset\\metadata.json\n",
      "2025-01-23 14:11:53 - INFO - Loading data from JSON file: ..\\resources\\data\\tokenized_dataset\\metadata.json\n",
      "2025-01-23 14:11:53 - INFO - Loading data from JSON file: ..\\resources\\data\\tokenized_dataset\\metadata.json\n",
      "2025-01-23 14:11:53 - INFO - Loading data from JSON file: ..\\resources\\data\\tokenized_dataset\\metadata.json\n",
      "2025-01-23 14:11:53 - INFO - Loaded JSON data from ..\\resources\\data\\tokenized_dataset\\metadata.json without pandas.\n",
      "2025-01-23 14:11:53 - INFO - Loaded JSON data from ..\\resources\\data\\tokenized_dataset\\metadata.json without pandas.\n",
      "2025-01-23 14:11:53 - INFO - Loaded JSON data from ..\\resources\\data\\tokenized_dataset\\metadata.json without pandas.\n",
      "2025-01-23 14:11:53 - INFO - Loaded JSON data from ..\\resources\\data\\tokenized_dataset\\metadata.json without pandas.\n",
      "2025-01-23 14:11:53 - INFO - Loaded JSON data from ..\\resources\\data\\tokenized_dataset\\metadata.json without pandas.\n",
      "2025-01-23 14:11:53 - INFO - Loaded JSON data from ..\\resources\\data\\tokenized_dataset\\metadata.json without pandas.\n",
      "2025-01-23 14:11:53 - INFO - Loaded JSON data from ..\\resources\\data\\tokenized_dataset\\metadata.json without pandas.\n",
      "2025-01-23 14:11:56 - INFO - Initializing model with model name: xlm-roberta-base and number of labels: 7\n",
      "2025-01-23 14:11:56 - INFO - Initializing model with model name: xlm-roberta-base and number of labels: 7\n",
      "2025-01-23 14:11:56 - INFO - Initializing model with model name: xlm-roberta-base and number of labels: 7\n",
      "2025-01-23 14:11:56 - INFO - Initializing model with model name: xlm-roberta-base and number of labels: 7\n",
      "2025-01-23 14:11:56 - INFO - Initializing model with model name: xlm-roberta-base and number of labels: 7\n",
      "2025-01-23 14:11:56 - INFO - Initializing model with model name: xlm-roberta-base and number of labels: 7\n",
      "2025-01-23 14:11:56 - INFO - Initializing model with model name: xlm-roberta-base and number of labels: 7\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-01-23 14:11:56 - INFO - Initializing data collator with tokenizer: XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "2025-01-23 14:11:56 - INFO - Initializing data collator with tokenizer: XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "2025-01-23 14:11:56 - INFO - Initializing data collator with tokenizer: XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "2025-01-23 14:11:56 - INFO - Initializing data collator with tokenizer: XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "2025-01-23 14:11:56 - INFO - Initializing data collator with tokenizer: XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "2025-01-23 14:11:56 - INFO - Initializing data collator with tokenizer: XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "2025-01-23 14:11:56 - INFO - Initializing data collator with tokenizer: XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "c:\\Users\\Darkles\\10academy\\env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Darkles\\10academy\\10Academy-Kifiya-Week-5\\notebooks\\..\\scripts\\modeling\\trainer.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "# Train without fine-tuning (full model training)\n",
    "trainer_base = train_ner_model(\n",
    "    data_path=splitted_dir,\n",
    "    model_name=MODEL_CHECKPOINT,\n",
    "    output_dir=os.path.join(model_output_dir, \"full_model\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the small evaluation set\n",
    "eval_results = trainer_base.evaluate()\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune without LoRA (traditional fine-tuning)\n",
    "trainer_no_lora = train_ner_model(\n",
    "    data_path=dataset_path,\n",
    "    model_name=MODEL_CHECKPOINT,\n",
    "    output_dir=os.path.join(model_output_dir, \"fine_tuned_without_lora\"),\n",
    "    use_peft=False,  # No PEFT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the small evaluation set\n",
    "eval_results = trainer_no_lora.evaluate()\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune with LoRA (parameter-efficient fine-tuning)\n",
    "trainer_lora = train_ner_model(\n",
    "    data_path=dataset_path,\n",
    "    model_name=MODEL_CHECKPOINT,\n",
    "    output_dir=os.path.join(model_output_dir, \"fine_tuned_with_lora\"),\n",
    "    use_peft=True,  # Enable PEFT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the small evaluation set\n",
    "eval_results = trainer_lora.evaluate()\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Fine tune multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameters\n",
    "models = [\n",
    "    \"xlm-roberta-base\",\n",
    "    \"Davlan/afroxlmr-large\",\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"AfroXLMR\",\n",
    "\n",
    "]\n",
    "\n",
    "# Fine-tune and compare models\n",
    "params = {\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 8,\n",
    "}\n",
    "\n",
    "# Fine-tune multiple models with LoRA\n",
    "trainers, results_df = fine_tune_multiple_models(\n",
    "    models=models,\n",
    "    dataset_dir=splitted_dir,\n",
    "    base_output_dir=model_output_dir,\n",
    "    params=params,\n",
    "    use_peft=True,\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabiulity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the label mapping\n",
    "label_map = {\n",
    "    0: \"O\", 1: \"B-PRICE\", 2: \"I-PRICE\", 3: \"B-LOC\", \n",
    "    4: \"I-LOC\", 5: \"B-PRODUCT\", 6: \"I-PRODUCT\"\n",
    "}\n",
    "\n",
    "# Fine-tuned model and dataset path\n",
    "model_name = \"resources/models/comparison/xlm-roberta-base\"\n",
    "dataset_path = \"resources/data/validation_data.txt\"\n",
    "example_idx = 5 \n",
    "\n",
    "# Generate explanation\n",
    "explain_predictions(model_name, dataset_path, example_idx, label_map)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
